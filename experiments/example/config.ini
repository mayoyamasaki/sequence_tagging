[path]
output_path=experiments/example/results/
model_output=${output_path}model.weights/
learning_curves_output=${output_path}lc.json
log_path=${output_path}log.txt

[data]
# embeddings
dim=50
dim_char=15
glove_filename=data/glove.6B/sampled-glove.6B.${dim}d.txt
# trimmed embeddings (created from glove_filename with build_data.py)
trimmed_filename=data/glove.6B.${dim}d.trimmed.npz

# support sequence data type iob and noniob
data_type=iob

dev_filename=data/conll2002-testa.iob
test_filename=data/conll2002-testb.iob
train_filename=data/conll2002-train.iob
# if not None, max number of examples
max_iter=None

# vocab (created from dataset with build_data.py)
words_filename=data/conll2002-words.txt
tags_filename=data/conll2002-tags.txt
chars_filename=data/cnoll2002-chars.txt

[hyperparameters]
# training
train_embeddings=False
nepochs=15
dropout=0.5
batch_size=20
lr_method=adam
lr=0.001
lr_decay=0.9
clip=-1
nepoch_no_imprv=3
reload=False
# model hyperparameters
hidden_size=300
char_hidden_size=100

# NOTE: if both chars and crf, only 1.6x slower on GPU
# if crf, training is 1.7x slower on CPU
crf=True
# if char embedding, training is 3.5x slower on CPU
chars=True
